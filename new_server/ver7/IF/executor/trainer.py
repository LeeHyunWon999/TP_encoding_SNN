import os
import torch
import numpy as np # .npy ì½ê¸°ìš©
import pandas as pd # csv ì½ê¸°ìš©
import torch.nn.functional as F  # ì¼ë¶€ í™œì„±í™” í•¨ìˆ˜ ë“± íŒŒë¼ë¯¸í„° ì—†ëŠ” í•¨ìˆ˜ì— ì‚¬ìš©
import torchvision.datasets as datasets  # ì¼ë°˜ì ì¸ ë°ì´í„°ì…‹; ì´ê±° ì•„ë§ˆ MIT-BIHë¡œ ë°”ê¿”ì•¼ í•  ë“¯?
import torchvision.transforms as transforms  # ë°ì´í„° ì¦ê°•ì„ ìœ„í•œ ì¼ì¢…ì˜ ë³€í˜•ì‘ì—…ì´ë¼ í•¨
from torch import optim  # SGD, Adam ë“±ì˜ ì˜µí‹°ë§ˆì´ì €(ê·¸ë˜ë””ì–¸íŠ¸ëŠ” ì´ìª½ìœ¼ë¡œ ê°€ë©´ ë©ë‹ˆë‹¤)
from torch.optim.lr_scheduler import CosineAnnealingLR # ì½”ì‚¬ì¸ìŠ¤ì¼€ì¤„ëŸ¬(ì˜µí‹°ë§ˆì´ì € ë³´ì¡°ìš©)
from torch import nn  # ëª¨ë“  DNN ëª¨ë¸ë“¤
from torch.utils.data import (DataLoader, Dataset)  # ë¯¸ë‹ˆë°°ì¹˜ ë“±ì˜ ë°ì´í„°ì…‹ ê´€ë¦¬ë¥¼ ë„ì™€ì£¼ëŠ” ë…€ì„
from tqdm import tqdm  # ì§„í–‰ë„ í‘œì‹œìš©
import torchmetrics # í‰ê°€ì§€í‘œ ë¡œê¹…ìš©
from typing import Callable # ëŒë‹¤ì‹
from torch.utils.tensorboard import SummaryWriter # tensorboard ê¸°ë¡ìš©
import time # í…ì„œë³´ë“œ í´ë”ëª…ì— ì“¸ ì‹œê°ì •ë³´ ê¸°ë¡ìš©
import random # ëœë¤ì‹œë“œ ê³ ì •ìš©

import sys
import json

# ì–˜ëŠ” SNN í•™ìŠµì´ë‹ˆê¹Œ ë‹¹ì—°íˆ ìˆì–´ì•¼ê² ì§€? íŠ¹íˆ SNN ëª¨ë¸ì„ ë”°ë¡œ ë§Œë“œë ¤ëŠ” ê²½ìš°ì—” ë‰´ëŸ° ë§ê³ ë„ ë„£ì„ ê²ƒì´ ë§ë‹¤.
# import spikingjelly.activation_based as jelly
from spikingjelly.activation_based import neuron, encoding, functional, surrogate, layer
from sklearn.model_selection import KFold # cross-validationìš©

from util import util

class trainer : 
    def __init__(self, args) -> None: 
        self.args = args

        # cuda í™˜ê²½ ì‚¬ìš©
        os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"  # GPU ë²ˆí˜¸ë³„ë¡œ 0ë²ˆë¶€í„° ë‚˜ì—´
        os.environ["CUDA_VISIBLE_DEVICES"]= str(args['device']['gpu'])  # ìƒí™©ì— ë§ì¶° ë³€ê²½í•  ê²ƒ
        self.device = "cuda" if torch.cuda.is_available() else "cpu" # ì—°ì‚°ì— GPU ì“°ë„ë¡ ì§€ì •
        print("Device :" + self.device) # í™•ì¸ìš©

        # ëœë¤ì‹œë“œ ê³ ì •
        seed = args['executor']['args']['random_seed']
        deterministic = True
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        if deterministic:
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False

        # íŒŒì¼ ì‹¤í–‰ ê¸°ì¤€ ì‹œê°„ ë³€ìˆ˜ ìƒì„±
        self.exec_time = time.strftime('%Y-%m-%d-%H-%M-%S')

        # í…ì„œë³´ë“œì— ì°ì„ ë©”íŠ¸ë¦­ ì—¬ê¸°ì„œ ì •ì˜
        self.f1_micro = torchmetrics.F1Score(num_classes=2, average='micro', task='binary').to(self.device)
        self.f1_weighted = torchmetrics.F1Score(num_classes=2, average='weighted', task='binary').to(self.device)
        self.auroc_macro = torchmetrics.AUROC(num_classes=2, average='macro', task='binary').to(self.device)
        self.auroc_weighted = torchmetrics.AUROC(num_classes=2, average='weighted', task='binary').to(self.device)
        self.auprc = torchmetrics.AveragePrecision(num_classes=2, task='binary').to(self.device)
        self.accuracy = torchmetrics.Accuracy(threshold=0.5, task='binary').to(self.device)

        # ì°¸ê³  : ì´ê²ƒ ì™¸ì—ë„ ì—í¬í¬, Lossê¹Œì§€ ì°ì–´ì•¼ í•˜ë‹ˆ ì°¸ê³ í•  ê²ƒ!
        self.earlystop_counter = args['executor']['args']['early_stop_epoch']
        self.min_valid_loss = float('inf')
        self.final_epoch = 0 # ë§ˆì§€ë§‰ì— ìµœì¢… ì—í¬í¬ í™•ì¸ìš©

        # ê°€ì¤‘ì¹˜ ë¹„ìœ¨ í…ì„œë¡œ ì˜®ê¸°ê¸°
        self.class_weight = torch.tensor(args['loss']['weight'], device=self.device)



    

    # í›ˆë ¨ ì‘ì—…(k-foldë¡œ ì§„í–‰)
    def train(self) : 
        args = self.args

        # ë°ì´í„°ì…‹ ë¡œë” ì„ ì • (ëª¨ë¸ì€ ê° fold ì•ˆì—ì„œ ì„ ì–¸í•˜ëŠ” í¸ì´ ë‚˜ì„ ë“¯..?)
        train_dataset = util.get_data_loader_train(args['data_loader'])

        # assert len(train_dataset) > 0, "ğŸš¨ CinC ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!"
        # print(f"âœ… Dataset size: {len(train_dataset)}")

        # temp_train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=False)
        # print(f"âœ… DataLoader batch count: {len(temp_train_loader)}")

        # k-fold ë°‘ì‘ì—…
        kf = KFold(n_splits=args['executor']['args']['k_folds'], shuffle=True, random_state=args['executor']['args']['random_seed'])

        # k-Fold ìˆ˜í–‰
        for fold, (train_idx, val_idx) in enumerate(kf.split(train_dataset)):
            print(f"Starting fold {fold + 1}/{args['executor']['args']['k_folds']}...")

            # Train/Validation ë°ì´í„°ì…‹ ë¶„ë¦¬
            train_subset = torch.utils.data.Subset(train_dataset, train_idx)
            val_subset = torch.utils.data.Subset(train_dataset, val_idx)

            train_loader = DataLoader(train_subset, batch_size=args['data_loader']['args']['batch_size'], 
                                      num_workers=args['data_loader']['args']['num_workers'],
                                      shuffle=True, drop_last=True)
            val_loader = DataLoader(val_subset, batch_size=args['data_loader']['args']['batch_size'], 
                                    num_workers=args['data_loader']['args']['num_workers'],
                                    shuffle=False)

            # TensorBoard í´ë” ì„¤ì •
            writer = SummaryWriter(log_dir=f"./tensorboard/{args['model']['type']}" + "_" + args['data_loader']['type'] + "_" + 
                                   self.exec_time + f"_fold{fold + 1}")

            # ì²´í¬í¬ì¸íŠ¸ ìœ„ì¹˜ë„ ìƒì„¸íˆ ê°±ì‹ 
            checkpoint_path_fold = args['executor']['args']['checkpoint']['path'] + str(str(args['model']['type']) + "_" + args['data_loader']['type'] + "_" + 
                                                                                        self.exec_time + f"_fold{fold + 1}")
            json_output_fold = checkpoint_path_fold + "_config.json" # ì²´í¬í¬ì¸íŠ¸ì— ë™ë´‰ë˜ëŠ” config ìš©
            lastpoint_path_fold = checkpoint_path_fold + "_lastEpoch.pt" # ìµœì¢…ì—í¬í¬ ì €ì¥ìš©
            checkpoint_path_fold += ".pt" # ì²´í¬í¬ì¸íŠ¸ í™•ì¥ì ë§ˆë¬´ë¦¬
            
            # SNN ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
            model = util.get_model(args['model'], device=self.device).to(device=self.device)

            # ì˜µí‹°ë§ˆì´ì €, ìŠ¤ì¼€ì¤„ëŸ¬
            train_params = [p for p in model.parameters() if p.requires_grad] # 'requires_grad'ê°€ Falseì¸ íŒŒë¼ë¯¸í„° ë§ê³  ë‚˜ë¨¸ì§€ëŠ” í•™ìŠµìš©ìœ¼ë¡œ ëŒë¦¬ê¸°ê¸°
            optimizer = util.get_optimizer(train_params, args['executor']['args']['optimizer'])
            scheduler = util.get_scheduler(optimizer, args['executor']['args']['scheduler'])

            # Training Loop
            for epoch in range(args['executor']['args']['num_epochs']):
                model.train()
                total_loss = 0
                self.accuracy.reset()
                self.f1_micro.reset()
                self.f1_weighted.reset()
                self.auroc_macro.reset()
                self.auroc_weighted.reset()
                self.auprc.reset()

                for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):
                    data = data.to(device=self.device).squeeze(1) # ì¼ì°¨ì›ì´ ìˆìœ¼ë©´ ì œê±°, ë”°ë¼ì„œ batchëŠ” ì ˆëŒ€ 1ë¡œ ë‘ë©´ ì•ˆë ë“¯
                    targets = targets.to(device=self.device)
                    label_onehot = F.one_hot(targets, args['model']['args']['num_classes']).float() # ì›í•«ìœ¼ë¡œ MSE loss ì“¸ê±°ì„

                    # ìˆœì „íŒŒ
                    out_fr = util.propagation(model, data, args['model'])

                    
                    loss = util.get_loss(out_fr, label_onehot, args['loss'])

                    weighted_loss = loss * self.class_weight[targets].unsqueeze(1)
                    final_loss = weighted_loss.mean()
                    total_loss += final_loss.item()

                    # ì—­ì „íŒŒ
                    optimizer.zero_grad()
                    final_loss.backward()
                    optimizer.step()

                    # ì§€í‘œ ê³„ì‚°
                    preds = torch.argmax(out_fr, dim=1)
                    self.accuracy.update(preds, targets)
                    self.f1_micro.update(preds, targets)
                    self.f1_weighted.update(preds, targets)
                    self.auroc_macro.update(preds, targets)
                    self.auroc_weighted.update(preds, targets)
                    probabilities = F.softmax(out_fr, dim=1)[:, 1]
                    self.auprc.update(probabilities, targets)

                    functional.reset_net(model)

                # ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” ì—í¬í¬ ë‹¨ìœ„ë¡œ ì§„í–‰
                scheduler.step()

                # í•œ ì—í¬í¬ ì§„í–‰ ë‹¤ ëìœ¼ë©´ training ì§€í‘œ tensorboardì— ì°ê³  valid ëŒë¦¬ê¸°
                train_loss = total_loss / len(train_loader)
                train_accuracy = self.accuracy.compute()
                train_f1_micro = self.f1_micro.compute()
                train_f1_weighted = self.f1_weighted.compute()
                train_auroc_macro = self.auroc_macro.compute()
                train_auroc_weighted = self.auroc_weighted.compute()
                train_auprc = self.auprc.compute()

                writer.add_scalar('train_Loss', train_loss, epoch)
                writer.add_scalar('train_Accuracy', train_accuracy, epoch)
                writer.add_scalar('train_F1_micro', train_f1_micro, epoch)
                writer.add_scalar('train_F1_weighted', train_f1_weighted, epoch)
                writer.add_scalar('train_AUROC_macro', train_auroc_macro, epoch)
                writer.add_scalar('train_AUROC_weighted', train_auroc_weighted, epoch)
                writer.add_scalar('train_auprc', train_auprc, epoch)

                # valid(ìì²´ì ìœ¼ë¡œ tensorboard ë‚´ì¥ë¨), ë°˜í™˜ê°’ìœ¼ë¡œ ì–¼ë¦¬ìŠ¤íƒ‘ í™•ì¸í•˜ê¸°
                valid_loss = self.valid(val_loader, model, writer, epoch)

                print(f"Fold {fold + 1}, Epoch {epoch + 1}/{args['executor']['args']['num_epochs']}, Valid Loss: {valid_loss}")

                # ì„±ëŠ¥ ì¢‹ê²Œ ë‚˜ì˜¤ë©´ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë° earlystop ê°±ì‹ 
                if args['executor']['args']['early_stop_enable'] :
                    if valid_loss < min_valid_loss : 
                        min_valid_loss = valid_loss
                        earlystop_counter = args['executor']['args']['early_stop_epoch']
                        if args['executor']['args']['checkpoint']['active'] : 
                            print("best performance, saving..")
                            torch.save({
                                'epoch': epoch,
                                'model_state_dict': model.state_dict(),
                                'optimizer_state_dict': optimizer.state_dict(),
                                'loss': valid_loss,
                                }, checkpoint_path_fold) # ê°€ì¥ ì¢‹ì€ ê¸°ë¡ ë‚˜ì˜¨ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                            with open(json_output_fold, "w", encoding='utf-8') as json_output : 
                                json.dump(args, json_output) # ì„¤ì •íŒŒì¼ë„ ì €ì¥
                    else : 
                        earlystop_counter -= 1
                        if earlystop_counter == 0 : # train epoch ë¹ ì ¸ë‚˜ì˜¤ë©° ìµœì¢… ëª¨ë¸ ì €ì¥
                            final_epoch = epoch
                            print("last epoch model saving..")
                            torch.save({
                                'epoch': final_epoch,
                                'model_state_dict': model.state_dict(),
                                'optimizer_state_dict': optimizer.state_dict(),
                                'loss': valid_loss,
                                }, lastpoint_path_fold)
                            with open(json_output_fold, "w", encoding='utf-8') as json_output : 
                                json.dump(args, json_output) # ì„¤ì •íŒŒì¼ë„ ì €ì¥
                            break # train epochë¥¼ ë¹ ì ¸ë‚˜ì˜´
                else : 
                    final_epoch = epoch
                    if epoch == args['executor']['args']['num_epochs'] - 1 : # ì–¼ë¦¬ìŠ¤íƒ‘ê³¼ ë³„ê°œë¡œ ìµœì¢… ëª¨ë¸ ì €ì¥
                        print("last epoch model saving..")
                        torch.save({
                            'epoch': final_epoch,
                            'model_state_dict': model.state_dict(),
                            'optimizer_state_dict': optimizer.state_dict(),
                            'loss': valid_loss,
                            }, lastpoint_path_fold)
                        with open(json_output_fold, "w", encoding='utf-8') as json_output : 
                                json.dump(args, json_output) # ì„¤ì •íŒŒì¼ë„ ì €ì¥


            # ê°œë³„ í…ì„œë³´ë“œ ë‹«ê¸°
            writer.close()

            print("fold " + f"{fold + 1}" + " training finished; epoch :" + str(final_epoch))

        print("All folds finished.")








    # ê²€ì¦ ì‘ì—…(validation), í…ŒìŠ¤íŠ¸ì™€ ë³„ê°œë¡œ epochë‹¹ 1íšŒì”© ì§„í–‰í•˜ê¸° (í›ˆë ¨ ë©”ì†Œë“œ ì™„ì„± í›„ ì§„í–‰í•˜ê¸°)
    def valid(self, loader, model, writer, epoch):
        args = self.args

        # ê°ì¢… ë©”íŠ¸ë¦­ë“¤ ë¦¬ì…‹(trainì—ì„œ ì—í­ë§ˆë‹¤ ëŒë¦¬ë¯€ë¡œ ì–˜ë„ ì—í­ë§ˆë‹¤ ë“¤ì–´ê°)
        total_loss = 0
        self.accuracy.reset()
        self.f1_micro.reset()
        self.f1_weighted.reset()
        self.auroc_macro.reset()
        self.auroc_weighted.reset()
        self.auprc.reset()

        # ëª¨ë¸ í‰ê°€ìš©ìœ¼ë¡œ ì „í™˜
        model.eval()
        
        print("validation ì§„í–‰ì¤‘...")

        with  torch.no_grad():
            for x, y in loader:         ############### trainìª½ì—ì„œ ì½”ë“œ ë³µë¶™ ì‹œ (data, targets) ê°€ (x, y) ë¡œ ë°”ë€ŒëŠ” ê²ƒì— ìœ ì˜í•  ê²ƒ!!!!!!!!###############
                x = x.to(device=self.device).squeeze(1)
                y = y.to(device=self.device)
                
                label_onehot = F.one_hot(y, args['model']['args']['num_classes']).float() # ì›í•«ìœ¼ë¡œ MSE loss ì“¸ê±°ì„
                
                # ìˆœì „íŒŒ
                out_fr = util.propagation(model, x, args['model'])

                loss = util.get_loss(out_fr, label_onehot, args['loss'])

                weighted_loss = loss * self.class_weight[y].unsqueeze(1) # ê°€ì¤‘ì¹˜ ê³±í•˜ê¸°
                final_loss = weighted_loss.mean() # ìš”ì†Œë³„ lossë¥¼ í‰ê· ë‚´ì„œ ì „ì²´ loss ê³„ì‚°
                
                # ì—¬ê¸°ì—ë„ total loss ì°ê¸°
                total_loss += final_loss.item()

                # ì—¬ê¸°ë„ ë©”íŠ¸ë¦­ updateí•´ì•¼ compute ê°€ëŠ¥í•¨
                # ì—¬ê¸°ë„ ë§ˆì°¬ê°€ì§€ë¡œ í¬ë¡œìŠ¤ì—”íŠ¸ë¡œí”¼ ë“œê°€ëŠ”ê±° ìƒê°í•´ì„œ 1ì°¨ì›ìœ¼ë¡œ ë³€ê²½ í•„ìš”í•¨
                preds = torch.argmax(out_fr, dim=1)
                self.accuracy.update(preds, y)
                self.f1_micro.update(preds, y)
                self.f1_weighted.update(preds, y)
                self.auroc_macro.update(preds, y)
                self.auroc_weighted.update(preds, y)
                probabilities = F.softmax(out_fr, dim=1)[:, 1]  # í´ë˜ìŠ¤ "1"ì˜ í™•ë¥  ì¶”ì¶œ
                self.auprc.update(probabilities, y)
                
                # ì–˜ë„ SNN ëª¨ë¸ì´ë‹ˆ ì´ˆê¸°í™” í•„ìš”
                functional.reset_net(model)

        # ê°ì¢… í‰ê°€ìˆ˜ì¹˜ë“¤ ë§Œë“¤ê³  tensorboardì— ê¸°ë¡
        valid_loss = total_loss / len(loader)
        valid_accuracy = self.accuracy.compute()
        valid_f1_micro = self.f1_micro.compute()
        valid_f1_weighted = self.f1_weighted.compute()
        valid_auroc_macro = self.auroc_macro.compute()
        valid_auroc_weighted = self.auroc_weighted.compute()
        valid_auprc = self.auprc.compute()

        writer.add_scalar('valid_Loss', valid_loss, epoch)
        writer.add_scalar('valid_Accuracy', valid_accuracy, epoch)
        writer.add_scalar('valid_F1_micro', valid_f1_micro, epoch)
        writer.add_scalar('valid_F1_weighted', valid_f1_weighted, epoch)
        writer.add_scalar('valid_AUROC_macro', valid_auroc_macro, epoch)
        writer.add_scalar('valid_AUROC_weighted', valid_auroc_weighted, epoch)
        writer.add_scalar('valid_auprc', valid_auprc, epoch)

        # ëª¨ë¸ ë‹¤ì‹œ í›ˆë ¨ìœ¼ë¡œ ì „í™˜
        model.train()

        # valid lossë¥¼ ë°˜í™˜í•œë‹¤. ì´ê±¸ë¡œ early stop í™•ì¸.
        return valid_loss