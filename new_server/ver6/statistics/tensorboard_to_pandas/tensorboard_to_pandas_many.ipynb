{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서보드로부터 값 뽑아내는 작업, 근데 이제 여러 텐서보드 로그를 취합하는.\n",
    "\n",
    "valid_AUROC 기준 최고성능인 epoch의 통계값들을 뽑아내면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 변수 넣기\n",
    "tensorboard_dirs_txt = '/home/hschoi/leehyunwon/ECG-SNN/new_server/ver6/statistics/tensorboard_to_pandas/encoding_main_test.txt'\n",
    "output_file_name = 'encoding_main_test.csv'\n",
    "\n",
    "tags = ['train_Loss','train_Accuracy','train_F1_micro','train_F1_weighted','train_AUROC_macro','train_AUROC_weighted','train_auprc', 'valid_Loss','valid_Accuracy','valid_F1_micro','valid_F1_weighted', 'valid_AUROC_macro','valid_AUROC_weighted','valid_auprc']\n",
    "usable_tags = ['valid_Accuracy','valid_F1_micro', 'valid_AUROC_macro', 'valid_auprc'] # 일반적인 경우\n",
    "# usable_tags = ['valid_AUROC_macro'] # stride 보려면 이것만 있어도 될 듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tensorboard_dirs_txt, 'r', encoding='utf-8') as file:\n",
    "    tensorboard_dirs = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing TensorBoard log: /home/hschoi/leehyunwon/ECG-SNN/new_server/ver6/IF/test/tensorboard/SNN_MLP_ver6_poisson_2024-12-27-13-02-25_test1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 18:06:57.043188: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-09 18:06:57.096686: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-09 18:06:57.896361: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag 'train_Loss' not found in the TensorBoard logs.\n",
      "Tag 'train_Accuracy' not found in the TensorBoard logs.\n",
      "Tag 'train_F1_micro' not found in the TensorBoard logs.\n",
      "Tag 'train_F1_weighted' not found in the TensorBoard logs.\n",
      "Tag 'train_AUROC_macro' not found in the TensorBoard logs.\n",
      "Tag 'train_AUROC_weighted' not found in the TensorBoard logs.\n",
      "Tag 'train_auprc' not found in the TensorBoard logs.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'train_Loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m dataframes \u001b[38;5;241m=\u001b[39m {tag: pd\u001b[38;5;241m.\u001b[39mDataFrame(values) \u001b[38;5;28;01mfor\u001b[39;00m tag, values \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# 단일 데이터프레임으로 병합 (Epoch 기준)\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mdataframes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m]})  \u001b[38;5;66;03m# 첫 태그의 epoch 사용\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m tags:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m dataframes:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'train_Loss'"
     ]
    }
   ],
   "source": [
    "# 병합된 데이터 저장용 리스트\n",
    "highest_rows = []\n",
    "\n",
    "for dir_path in tensorboard_dirs: # 각 로그마다\n",
    "    print(f\"Processing TensorBoard log: {dir_path}\")\n",
    "    ea = event_accumulator.EventAccumulator(dir_path)\n",
    "    ea.Reload() # 텐서보드 로그 뽑아서 변수에 넣기기\n",
    "    \n",
    "    data = {} # 수집된 데이터를 저장할 딕셔너리\n",
    "    epoch_time_map = {}  # 에포크와 wall_time 매핑\n",
    "\n",
    "    # 태그(메트릭) 별로 에포크와 값 빼내서 저장\n",
    "    for tag in tags:\n",
    "        if tag in ea.Tags()[\"scalars\"]:  # 태그가 존재하는지 확인\n",
    "            events = ea.Scalars(tag)\n",
    "            data[tag] = {\n",
    "                \"epoch\": [event.step for event in events],  # step을 epoch으로 사용\n",
    "                \"value\": [event.value for event in events],\n",
    "            }\n",
    "            # epoch -> wall_time 매핑 (처음 태그에서만 추출)\n",
    "            if not epoch_time_map:\n",
    "                epoch_time_map = {event.step: event.wall_time for event in events}\n",
    "        else:\n",
    "            print(f\"Tag '{tag}' not found in the TensorBoard logs.\")\n",
    "\n",
    "    # 태그별 데이터프레임 생성\n",
    "    dataframes = {tag: pd.DataFrame(values) for tag, values in data.items()}\n",
    "\n",
    "    # 단일 데이터프레임으로 병합 (Epoch 기준)\n",
    "    merged_df = pd.DataFrame({\"epoch\": dataframes[tags[0]][\"epoch\"]})  # 첫 태그의 epoch 사용\n",
    "    for tag in tags:\n",
    "        if tag in dataframes:\n",
    "            merged_df[tag] = dataframes[tag][\"value\"]\n",
    "\n",
    "    # 필요한 태그로 필터링\n",
    "    filtered_df = merged_df[['epoch'] + [tag for tag in usable_tags if tag in merged_df.columns]] # 에포크도 보고 싶으니 추가\n",
    "\n",
    "    # 가장 높은 AUROC 값을 갖는 행 추출\n",
    "    if \"valid_AUROC_macro\" in filtered_df.columns:\n",
    "        highest_row = filtered_df.loc[filtered_df['valid_AUROC_macro'].idxmax()].copy()  # 명시적 복사\n",
    "        highest_row[\"source_dir\"] = dir_path.split('/')[-1]  # 로그 출처 추가\n",
    "\n",
    "        # 학습 시간 계산\n",
    "        highest_epoch = int(highest_row['epoch'])\n",
    "        if highest_epoch in epoch_time_map:\n",
    "            start_time = min(epoch_time_map.values())  # 첫 epoch 시작 시간\n",
    "            end_time = epoch_time_map[highest_epoch]  # 최고 성능 epoch의 시간\n",
    "            elapsed_time = (end_time - start_time) / 60 # 경과 시간 계산 (분)\n",
    "            highest_row[\"total elapsed minute\"] = elapsed_time\n",
    "            highest_row[\"minute per epoch\"] = elapsed_time / highest_row['epoch']\n",
    "        else:\n",
    "            highest_row[\"total elapsed minute\"] = None\n",
    "            highest_row[\"minute per epoch\"] = None  # 해당 에포크 시간 데이터 없음\n",
    "\n",
    "        highest_rows.append(highest_row)\n",
    "\n",
    "# 최종 병합\n",
    "final_df = pd.DataFrame(highest_rows)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"\\nFinal DataFrame with Highest AUROC Rows:\")\n",
    "print(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장하기\n",
    "final_df.to_csv(output_file_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg_encoding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
